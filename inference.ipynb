{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnQRYjPVB0kBEybSLa1xaL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnavdesai6143/Datathon_TM126/blob/main/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWgnIiKD9-pP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "CTG Fetal Distress Classification - Testing/Inference Script\n",
        "Loads trained models and evaluates on test data\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    balanced_accuracy_score, f1_score, classification_report,\n",
        "    confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
        ")\n",
        "\n",
        "# Load test data (same split as training)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cleaned_df = pd.read_csv('ctg_cleaned.csv')\n",
        "X = cleaned_df.drop(columns=['NSP'], errors='ignore')\n",
        "y = cleaned_df['NSP'].astype(int)\n",
        "X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD TRAINED MODELS AND EVALUATE\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL TEST SET EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_results = []\n",
        "class_names = ['Normal', 'Suspect', 'Pathologic']\n",
        "\n",
        "# Load all trained models\n",
        "model_names = ['Logistic_Regression', 'Random_Forest', 'Gradient_Boosting', 'MLP_Neural_Network']\n",
        "\n",
        "for name in model_names:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Evaluating: {name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Load model\n",
        "    estimator = joblib.load(f'models/{name}_model.pkl')\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = estimator.predict(X_test)\n",
        "\n",
        "    # Core metrics\n",
        "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nðŸ“Š  Test Set Performance:\")\n",
        "    print(f\"  {'Balanced Accuracy:':<25} {bal_acc:.4f}\")\n",
        "    print(f\"  {'Macro F1-Score:':<25} {macro_f1:.4f}\")\n",
        "    print(f\"  {'Weighted F1-Score:':<25} {weighted_f1:.4f}\")\n",
        "\n",
        "    # Per-class metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_test, y_pred, labels=[1, 2, 3], zero_division=0\n",
        "    )\n",
        "\n",
        "    print(\"\\nðŸ“ˆ  Per-Class Performance:\")\n",
        "    print(f\"  {'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\n",
        "    print(\"  \" + \"-\"*60)\n",
        "    for i, cls_name in enumerate(class_names):\n",
        "        print(f\"  {cls_name:<15} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1[i]:<12.4f} {support[i]:<12}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\n\" + classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=[1, 2, 3])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
        "    ax.set_title(f'{name} - Confusion Matrix\\n' +\n",
        "                 f'Balanced Acc: {bal_acc:.4f} | Macro F1: {macro_f1:.4f}',\n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Store results\n",
        "    test_results.append({\n",
        "        'Model': name,\n",
        "        'Test_Balanced_Accuracy': float(bal_acc),\n",
        "        'Test_Macro_F1': float(macro_f1),\n",
        "        'Class_3_Recall': float(recall[2])\n",
        "    })\n",
        "\n",
        "# Display test results summary\n",
        "test_results_df = pd.DataFrame(test_results).sort_values('Test_Macro_F1', ascending=False)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST SET RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(test_results_df.to_string(index=False))\n",
        "\n",
        "test_results_df.to_csv('test_results.csv', index=False)\n",
        "print(\"\\nâœ“ Test results saved to: test_results.csv\")"
      ]
    }
  ]
}